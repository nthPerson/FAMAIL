"""Dataset loading utilities for the discriminator model.

Provides PyTorch Dataset classes for loading trajectory pair data generated
by the dataset_generation_tool.

Expected input format (.npz files):
    - x1: First trajectories [N, L, 4]
    - x2: Second trajectories [N, L, 4]
    - label: Binary labels [N] where 1=same agent, 0=different agent
    - mask1: Boolean masks for x1 [N, L]
    - mask2: Boolean masks for x2 [N, L]
    
Features (4): [x_grid, y_grid, time_bucket, day_index]
"""

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from typing import Dict, List, Optional, Tuple, Union
from pathlib import Path


class TrajectoryPairDataset(Dataset):
    """PyTorch Dataset for trajectory pairs.
    
    Loads data from .npz files generated by the dataset_generation_tool.
    """
    
    def __init__(self, 
                 data_path: Union[str, Path],
                 transform: Optional[callable] = None):
        """Initialize the dataset.
        
        Args:
            data_path: Path to .npz file containing the dataset
            transform: Optional transform to apply to data
        """
        self.data_path = Path(data_path)
        self.transform = transform
        
        # Load data
        with np.load(self.data_path) as data:
            self.x1 = data['x1'].astype(np.float32)
            self.x2 = data['x2'].astype(np.float32)
            self.labels = data['label'].astype(np.float32)
            self.mask1 = data['mask1'].astype(bool)
            self.mask2 = data['mask2'].astype(bool)
            
        self.n_samples = len(self.labels)
        
        # Compute statistics
        self._compute_stats()
        
    def _compute_stats(self):
        """Compute dataset statistics."""
        self.n_positive = int((self.labels == 1).sum())
        self.n_negative = int((self.labels == 0).sum())
        self.seq_len = self.x1.shape[1]
        self.n_features = self.x1.shape[2]
        
        # Compute average sequence lengths
        self.avg_len1 = float(self.mask1.sum(axis=1).mean())
        self.avg_len2 = float(self.mask2.sum(axis=1).mean())
        
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """Get a single trajectory pair.
        
        Returns:
            Dict with keys: 'x1', 'x2', 'mask1', 'mask2', 'label'
        """
        item = {
            'x1': torch.from_numpy(self.x1[idx]),
            'x2': torch.from_numpy(self.x2[idx]),
            'mask1': torch.from_numpy(self.mask1[idx]),
            'mask2': torch.from_numpy(self.mask2[idx]),
            'label': torch.tensor(self.labels[idx], dtype=torch.float32)
        }
        
        if self.transform:
            item = self.transform(item)
            
        return item
    
    def get_stats(self) -> Dict:
        """Get dataset statistics."""
        return {
            'n_samples': self.n_samples,
            'n_positive': self.n_positive,
            'n_negative': self.n_negative,
            'positive_ratio': self.n_positive / self.n_samples,
            'seq_len': self.seq_len,
            'n_features': self.n_features,
            'avg_len1': self.avg_len1,
            'avg_len2': self.avg_len2
        }
        
    def __repr__(self) -> str:
        return (f"TrajectoryPairDataset(n_samples={self.n_samples}, "
                f"n_positive={self.n_positive}, n_negative={self.n_negative}, "
                f"seq_len={self.seq_len})")


class MultiFileDataset(Dataset):
    """Dataset that loads from multiple .npz files.
    
    Useful for combining multiple dataset files (e.g., train/val from different sources).
    """
    
    def __init__(self, 
                 data_paths: List[Union[str, Path]],
                 transform: Optional[callable] = None):
        """Initialize from multiple files.
        
        Args:
            data_paths: List of paths to .npz files
            transform: Optional transform to apply
        """
        self.data_paths = [Path(p) for p in data_paths]
        self.transform = transform
        
        # Load and concatenate all data
        x1_list, x2_list, labels_list, mask1_list, mask2_list = [], [], [], [], []
        
        for path in self.data_paths:
            with np.load(path) as data:
                x1_list.append(data['x1'].astype(np.float32))
                x2_list.append(data['x2'].astype(np.float32))
                labels_list.append(data['label'].astype(np.float32))
                mask1_list.append(data['mask1'].astype(bool))
                mask2_list.append(data['mask2'].astype(bool))
                
        # Concatenate (will fail if shapes don't match on non-batch dims)
        self.x1 = np.concatenate(x1_list, axis=0)
        self.x2 = np.concatenate(x2_list, axis=0)
        self.labels = np.concatenate(labels_list, axis=0)
        self.mask1 = np.concatenate(mask1_list, axis=0)
        self.mask2 = np.concatenate(mask2_list, axis=0)
        
        self.n_samples = len(self.labels)
        
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        item = {
            'x1': torch.from_numpy(self.x1[idx]),
            'x2': torch.from_numpy(self.x2[idx]),
            'mask1': torch.from_numpy(self.mask1[idx]),
            'mask2': torch.from_numpy(self.mask2[idx]),
            'label': torch.tensor(self.labels[idx], dtype=torch.float32)
        }
        
        if self.transform:
            item = self.transform(item)
            
        return item


def load_dataset_from_directory(
    directory: Union[str, Path],
    train_file: str = "train.npz",
    val_file: str = "val.npz",
    test_file: Optional[str] = "test.npz"
) -> Dict[str, TrajectoryPairDataset]:
    """Load train/val/test datasets from a directory.
    
    Expected directory structure:
        directory/
            train.npz
            val.npz
            test.npz (optional)
            
    Args:
        directory: Path to dataset directory
        train_file: Filename for training data
        val_file: Filename for validation data
        test_file: Filename for test data (None to skip)
        
    Returns:
        Dict with 'train', 'val', and optionally 'test' datasets
    """
    directory = Path(directory)
    
    datasets = {}
    
    train_path = directory / train_file
    if train_path.exists():
        datasets['train'] = TrajectoryPairDataset(train_path)
    else:
        raise FileNotFoundError(f"Training file not found: {train_path}")
        
    val_path = directory / val_file
    if val_path.exists():
        datasets['val'] = TrajectoryPairDataset(val_path)
    else:
        raise FileNotFoundError(f"Validation file not found: {val_path}")
        
    if test_file:
        test_path = directory / test_file
        if test_path.exists():
            datasets['test'] = TrajectoryPairDataset(test_path)
            
    return datasets


def create_train_val_split(
    dataset: TrajectoryPairDataset,
    val_ratio: float = 0.2,
    seed: int = 42
) -> Tuple[Dataset, Dataset]:
    """Split a dataset into train and validation sets.
    
    Args:
        dataset: Dataset to split
        val_ratio: Fraction for validation
        seed: Random seed for reproducibility
        
    Returns:
        Tuple of (train_dataset, val_dataset)
    """
    n_val = int(len(dataset) * val_ratio)
    n_train = len(dataset) - n_val
    
    generator = torch.Generator().manual_seed(seed)
    train_dataset, val_dataset = random_split(
        dataset, [n_train, n_val], generator=generator
    )
    
    return train_dataset, val_dataset


def create_data_loaders(
    train_dataset: Dataset,
    val_dataset: Dataset,
    batch_size: int = 32,
    num_workers: int = 0,
    pin_memory: bool = True
) -> Tuple[DataLoader, DataLoader]:
    """Create DataLoaders for training and validation.
    
    Args:
        train_dataset: Training dataset
        val_dataset: Validation dataset
        batch_size: Batch size
        num_workers: Number of worker processes
        pin_memory: Whether to pin memory for GPU transfer
        
    Returns:
        Tuple of (train_loader, val_loader)
    """
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    return train_loader, val_loader


# Data augmentation transforms (optional)

class RandomSwapPair:
    """Randomly swap x1 and x2 in a pair (data augmentation).
    
    Since same-agent pairs are symmetric, swapping maintains the label.
    For different-agent pairs, swapping also maintains the label.
    """
    
    def __init__(self, p: float = 0.5):
        self.p = p
        
    def __call__(self, item: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        if torch.rand(1).item() < self.p:
            return {
                'x1': item['x2'],
                'x2': item['x1'],
                'mask1': item['mask2'],
                'mask2': item['mask1'],
                'label': item['label']
            }
        return item


class AddGaussianNoise:
    """Add Gaussian noise to spatial features (data augmentation).
    
    Only adds noise to x_grid and y_grid (indices 0 and 1).
    Noise is added before normalization, so values should be in grid units.
    """
    
    def __init__(self, std: float = 0.1):
        """
        Args:
            std: Standard deviation of noise in grid units
        """
        self.std = std
        
    def __call__(self, item: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        # Only add noise to spatial features (x_grid, y_grid)
        noise1 = torch.zeros_like(item['x1'])
        noise2 = torch.zeros_like(item['x2'])
        
        noise1[:, :2] = torch.randn(item['x1'].shape[0], 2) * self.std
        noise2[:, :2] = torch.randn(item['x2'].shape[0], 2) * self.std
        
        # Apply noise only to valid timesteps
        mask1 = item['mask1'].unsqueeze(-1).float()
        mask2 = item['mask2'].unsqueeze(-1).float()
        
        item['x1'] = item['x1'] + noise1 * mask1
        item['x2'] = item['x2'] + noise2 * mask2
        
        # Clamp to valid ranges
        item['x1'][:, 0] = item['x1'][:, 0].clamp(0, 49)
        item['x1'][:, 1] = item['x1'][:, 1].clamp(0, 89)
        item['x2'][:, 0] = item['x2'][:, 0].clamp(0, 49)
        item['x2'][:, 1] = item['x2'][:, 1].clamp(0, 89)
        
        return item


class Compose:
    """Compose multiple transforms."""
    
    def __init__(self, transforms: List[callable]):
        self.transforms = transforms
        
    def __call__(self, item: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        for t in self.transforms:
            item = t(item)
        return item
