{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3035393",
   "metadata": {},
   "source": [
    "## Data Sample Creation Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5ebaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, random, math, pathlib\n",
    "from typing import Any, Iterable\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b442b6",
   "metadata": {},
   "source": [
    "#### General Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba1b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_pickle_load(path: str):\n",
    "    \"\"\"\n",
    "    WARNING: pickle.load executes code on deserialization. Only load files you trust.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    pathlib.Path(p).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sizeof_bytes(obj: Any) -> int:\n",
    "    try:\n",
    "        return len(json.dumps(obj))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def truncate_str(s: str, max_len=160):\n",
    "    return s if len(s) <= max_len else s[:max_len] + \"…\"\n",
    "\n",
    "def to_jsonable(x, max_depth=10, max_list=50, max_dict=50, max_str=160, depth=0):\n",
    "    \"\"\"\n",
    "    Convert arbitrary Python objects into JSON-serializable samples,\n",
    "    limiting depth, container sizes, and string length.\n",
    "    \"\"\"\n",
    "    if depth >= max_depth:\n",
    "        return f\"<…depth={depth} truncated…>\"\n",
    "    if isinstance(x, (str, int, float, bool)) or x is None:\n",
    "        return truncate_str(x, max_str) if isinstance(x, str) else x\n",
    "    if isinstance(x, (np.integer, np.floating, np.bool_)):\n",
    "        return x.item()\n",
    "    if isinstance(x, (bytes, bytearray, memoryview)):\n",
    "        return f\"<{type(x).__name__} len={len(x)}>\"\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        out = []\n",
    "        for i, v in enumerate(x[:max_list]):\n",
    "            out.append(to_jsonable(v, max_depth, max_list, max_dict, max_str, depth+1))\n",
    "        if len(x) > max_list:\n",
    "            out.append(f\"<…{len(x)-max_list} more…>\")\n",
    "        return out\n",
    "    if isinstance(x, set):\n",
    "        x = list(x)\n",
    "        return to_jsonable(x, max_depth, max_list, max_dict, max_str, depth)\n",
    "    if isinstance(x, dict):\n",
    "        out = {}\n",
    "        for i, (k, v) in enumerate(list(x.items())[:max_dict]):\n",
    "            kk = str(k)\n",
    "            out[truncate_str(kk, max_str)] = to_jsonable(v, max_depth, max_list, max_dict, max_str, depth+1)\n",
    "        if len(x) > max_dict:\n",
    "            out[\"<…more_keys…>\"] = len(x)-max_dict\n",
    "        return out\n",
    "    # numpy arrays\n",
    "    if isinstance(x, np.ndarray):\n",
    "        shape = list(x.shape)\n",
    "        head = x\n",
    "        if x.ndim == 1:\n",
    "            head = x[: min(len(x), 50)]\n",
    "        elif x.ndim >= 2:\n",
    "            head = x[: min(x.shape[0], 20), : min(x.shape[1], 20)]\n",
    "        return {\"__ndarray__\": True, \"dtype\": str(x.dtype), \"shape\": shape, \"preview\": head.tolist()}\n",
    "    # pandas\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        return {\n",
    "            \"__dataframe__\": True,\n",
    "            \"shape\": list(x.shape),\n",
    "            \"dtypes\": {c: str(t) for c, t in x.dtypes.items()},\n",
    "            \"head\": json.loads(x.head(10).to_json(orient=\"records\"))\n",
    "        }\n",
    "    if isinstance(x, pd.Series):\n",
    "        return {\n",
    "            \"__series__\": True,\n",
    "            \"name\": str(x.name),\n",
    "            \"dtype\": str(x.dtype),\n",
    "            \"head\": json.loads(x.head(20).to_json(orient=\"records\"))\n",
    "        }\n",
    "    # fallback\n",
    "    return f\"<{type(x).__name__}>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff1f49",
   "metadata": {},
   "source": [
    "#### DataFrame-specific sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65225b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataframe(df: pd.DataFrame,\n",
    "                     target_bytes: int = 1_500_000,\n",
    "                     max_rows: int = 3000,\n",
    "                     random_state: int = 42) -> pd.DataFrame:\n",
    "    if len(df) == 0:\n",
    "        return df.copy()\n",
    "\n",
    "    # Estimate bytes per row (fallback if deep fails)\n",
    "    try:\n",
    "        approx = df.head(min(2000, len(df))).memory_usage(deep=True).sum()\n",
    "        est_bpr = max(1, approx / min(2000, len(df)))\n",
    "    except Exception:\n",
    "        est_bpr = max(1, (df.memory_usage().sum() / max(1, len(df))))\n",
    "\n",
    "    n_by_size = int(target_bytes / est_bpr)\n",
    "    n = max(1, min(max_rows, n_by_size, len(df)))\n",
    "\n",
    "    if n == len(df):\n",
    "        return df.copy()\n",
    "\n",
    "    # Prefer stratified-ish sample if a categorical-like column exists\n",
    "    cat_cols = [c for c in df.columns if str(df[c].dtype) == \"category\" or df[c].nunique() <= 50]\n",
    "    if cat_cols:\n",
    "        # Simple group-wise sample across the first available categorical column\n",
    "        key = cat_cols[0]\n",
    "        frac = n / len(df)\n",
    "        try:\n",
    "            return df.groupby(key, group_keys=False).apply(lambda g: g.sample(max(1, int(math.ceil(len(g)*frac))), random_state=random_state)).head(n)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return df.sample(n=n, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3bdf2",
   "metadata": {},
   "source": [
    "#### Write “mini-pickle” + “LLM pack” (JSONL + schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94aed7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_llm_pack_from_df(df: pd.DataFrame, out_stem: str, string_max_len=200):\n",
    "    \"\"\"\n",
    "    Writes two artifacts for a DataFrame:\n",
    "      1) JSONL with normalized, truncated records: <out_stem>.jsonl\n",
    "      2) Plain-text schema/preview (no code fences): <out_stem>.schema.md\n",
    "    \"\"\"\n",
    "\n",
    "    def normalize_value(v):\n",
    "        if isinstance(v, str):\n",
    "            return truncate_str(v, string_max_len)\n",
    "        if isinstance(v, (np.integer, np.floating, np.bool_)):\n",
    "            return v.item()\n",
    "        if isinstance(v, (list, tuple, dict, set, np.ndarray, pd.Series, pd.Timestamp)):\n",
    "            try:\n",
    "                return to_jsonable(v, max_depth=10, max_list=10, max_dict=10, max_str=string_max_len)\n",
    "            except Exception:\n",
    "                return str(v)[:string_max_len]\n",
    "        return v\n",
    "\n",
    "    # Records -> JSONL\n",
    "    records = []\n",
    "    for rec in df.to_dict(orient=\"records\"):\n",
    "        records.append({k: normalize_value(v) for k, v in rec.items()})\n",
    "\n",
    "    jsonl_path = f\"{out_stem}.jsonl\"\n",
    "    ensure_dir(jsonl_path)\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Build dtypes table (markdown-like but plain text; no fences)\n",
    "    dtypes_df = pd.DataFrame({\"column\": df.columns, \"dtype\": df.dtypes.astype(str).values})\n",
    "    try:\n",
    "        dtypes_md = dtypes_df.to_markdown(index=False)\n",
    "    except Exception:\n",
    "        # Fallback simple table\n",
    "        header = \"column | dtype\\n---|---\\n\"\n",
    "        rows = \"\\n\".join(f\"{c} | {t}\" for c, t in zip(dtypes_df[\"column\"], dtypes_df[\"dtype\"]))\n",
    "        dtypes_md = header + rows\n",
    "\n",
    "    example_rows_text = json.dumps(records[:5], ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Plain-text schema (no backticks, no code fences)\n",
    "    schema_lines = []\n",
    "    schema_lines.append(\"Schema for: \" + os.path.basename(out_stem))\n",
    "    schema_lines.append(\"\")\n",
    "    schema_lines.append(f\"Rows: {len(df)}\")\n",
    "    schema_lines.append(f\"Columns ({len(df.columns)}): \" + \", \".join(map(str, df.columns)))\n",
    "    schema_lines.append(\"\")\n",
    "    schema_lines.append(\"dtypes table:\")\n",
    "    schema_lines.append(dtypes_md)\n",
    "    schema_lines.append(\"\")\n",
    "    schema_lines.append(\"Example rows (first 5) as JSON:\")\n",
    "    schema_lines.append(example_rows_text)\n",
    "    schema_lines.append(\"\")\n",
    "\n",
    "    md_path = f\"{out_stem}.schema.md\"\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(schema_lines))\n",
    "\n",
    "\n",
    "def write_llm_pack_from_obj(obj: Any, out_stem: str):\n",
    "    \"\"\"\n",
    "    Writes two artifacts for an arbitrary Python object:\n",
    "      1) JSON preview with bounds on nesting/size: <out_stem>.json\n",
    "      2) Plain-text structure note (no code fences): <out_stem>.schema.md\n",
    "    \"\"\"\n",
    "    preview = to_jsonable(obj, max_depth=5, max_list=130, max_dict=100)\n",
    "\n",
    "    json_path = f\"{out_stem}.json\"\n",
    "    ensure_dir(json_path)\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(preview, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    schema_lines = []\n",
    "    schema_lines.append(\"Structure for: \" + os.path.basename(out_stem))\n",
    "    schema_lines.append(\"Type: \" + type(obj).__name__)\n",
    "    schema_lines.append(\"Approx JSON size (chars): ~\" + str(sizeof_bytes(preview)))\n",
    "    schema_lines.append(\"\")\n",
    "\n",
    "    md_path = f\"{out_stem}.schema.md\"\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(schema_lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a9847",
   "metadata": {},
   "source": [
    "#### Main sampler (works for DataFrames, lists, dicts, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e245f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pickle_sample(\n",
    "    input_path: str,\n",
    "    output_dir: str = \"samples\",\n",
    "    target_bytes: int = 1_500_000,       # ~1.5 MB preview target\n",
    "    df_max_rows: int = 3000,\n",
    "    mini_pickle_name: str | None = None,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    obj = safe_pickle_load(input_path)\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    out_stem = os.path.join(output_dir, base)\n",
    "\n",
    "    # Case A: pandas DataFrame\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        sdf = sample_dataframe(obj, target_bytes=target_bytes, max_rows=df_max_rows, random_state=random_state)\n",
    "        # 1) mini-pickle\n",
    "        pkl_path = f\"{out_stem}.mini.pkl\" if mini_pickle_name is None else os.path.join(output_dir, mini_pickle_name)\n",
    "        ensure_dir(pkl_path)\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(sdf, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # 2) LLM pack\n",
    "        write_llm_pack_from_df(sdf, out_stem)\n",
    "        return {\"type\": \"dataframe\", \"mini_pickle\": pkl_path, \"llm_jsonl\": f\"{out_stem}.jsonl\", \"schema\": f\"{out_stem}.schema.md\", \"rows\": len(sdf)}\n",
    "\n",
    "    # Case B: list/tuple -> sample slice + a bit of randomness\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        k = min(len(obj), 2000)\n",
    "        head = list(obj[: min(1000, len(obj))])\n",
    "        remainder = obj[min(1000, len(obj)):]\n",
    "        if remainder:\n",
    "            rnd_take = min(len(remainder), max(0, 1000))\n",
    "            random.seed(random_state)\n",
    "            head += random.sample(remainder, rnd_take)\n",
    "        mini = head[:k]\n",
    "        # 1) mini-pickle\n",
    "        pkl_path = f\"{out_stem}.mini.pkl\"\n",
    "        ensure_dir(pkl_path)\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(mini, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # 2) LLM pack\n",
    "        write_llm_pack_from_obj(mini, out_stem)\n",
    "        return {\"type\": type(obj).__name__, \"mini_pickle\": pkl_path, \"llm_json\": f\"{out_stem}.json\", \"schema\": f\"{out_stem}.schema.md\", \"items\": len(mini)}\n",
    "\n",
    "    # Case C: dict -> keep up to 1000 keys (head + random)\n",
    "    if isinstance(obj, dict):\n",
    "        items = list(obj.items())\n",
    "        head = items[: 1]\n",
    "        # head = items[: min(2, len(items))]\n",
    "        # remainder = items[min(2, len(items)):]\n",
    "        # if remainder:\n",
    "        #     random.seed(random_state)\n",
    "        #     head += random.sample(remainder, min(2, len(remainder)))\n",
    "        mini = dict(head)\n",
    "        pkl_path = f\"{out_stem}.mini.pkl\"\n",
    "        ensure_dir(pkl_path)\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(mini, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        write_llm_pack_from_obj(mini, out_stem)\n",
    "        return {\"type\": \"dict\", \"mini_pickle\": pkl_path, \"llm_json\": f\"{out_stem}.json\", \"schema\": f\"{out_stem}.schema.md\", \"keys\": len(mini)}\n",
    "    # if isinstance(obj, dict):\n",
    "    #     items = list(obj.items())\n",
    "    #     head = items[: min(600, len(items))]\n",
    "    #     remainder = items[min(600, len(items)):]\n",
    "    #     if remainder:\n",
    "    #         random.seed(random_state)\n",
    "    #         head += random.sample(remainder, min(400, len(remainder)))\n",
    "    #     mini = dict(head)\n",
    "    #     pkl_path = f\"{out_stem}.mini.pkl\"\n",
    "    #     ensure_dir(pkl_path)\n",
    "    #     with open(pkl_path, \"wb\") as f:\n",
    "    #         pickle.dump(mini, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #     write_llm_pack_from_obj(mini, out_stem)\n",
    "    #     return {\"type\": \"dict\", \"mini_pickle\": pkl_path, \"llm_json\": f\"{out_stem}.json\", \"schema\": f\"{out_stem}.schema.md\", \"keys\": len(mini)}\n",
    "\n",
    "    # Case D: anything else -> store original as mini if already small, else JSON preview only\n",
    "    pkl_path = f\"{out_stem}.mini.pkl\"\n",
    "    ensure_dir(pkl_path)\n",
    "    try:\n",
    "        # Try dumping as-is (object might already be small, e.g., train_airport.pkl)\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        dumped = True\n",
    "    except Exception:\n",
    "        dumped = False\n",
    "\n",
    "    write_llm_pack_from_obj(obj, out_stem)\n",
    "    return {\"type\": type(obj).__name__, \"mini_pickle\": pkl_path if dumped else None, \"llm_json\": f\"{out_stem}.json\", \"schema\": f\"{out_stem}.schema.md\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6745d",
   "metadata": {},
   "source": [
    "#### Run sampler on processed files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3ac6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latest_traffic.pkl': {'type': 'dict',\n",
       "  'mini_pickle': 'samples/latest_traffic.mini.pkl',\n",
       "  'llm_json': 'samples/latest_traffic.json',\n",
       "  'schema': 'samples/latest_traffic.schema.md',\n",
       "  'keys': 1000},\n",
       " 'latest_volume_pickups.pkl': {'type': 'dict',\n",
       "  'mini_pickle': 'samples/latest_volume_pickups.mini.pkl',\n",
       "  'llm_json': 'samples/latest_volume_pickups.json',\n",
       "  'schema': 'samples/latest_volume_pickups.schema.md',\n",
       "  'keys': 1000},\n",
       " 'train_airport.pkl': {'type': 'dict',\n",
       "  'mini_pickle': 'samples/train_airport.mini.pkl',\n",
       "  'llm_json': 'samples/train_airport.json',\n",
       "  'schema': 'samples/train_airport.schema.md',\n",
       "  'keys': 21}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [\n",
    "    \"latest_traffic.pkl\",         # ~730 MB\n",
    "    \"latest_volume_pickups.pkl\",  # ~223 MB\n",
    "    \"train_airport.pkl\",          # ~2 KB\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for p in inputs:\n",
    "    results[p] = make_pickle_sample(\n",
    "        p,\n",
    "        output_dir=\"samples\",\n",
    "        target_bytes=1_500_000,   # ~1.5 MB preview target (adjust up/down)\n",
    "        df_max_rows=3000,         # hard cap on rows for DataFrames\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f107b8c",
   "metadata": {},
   "source": [
    "#### Run sampler on all_trajs.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b807ee8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'dict',\n",
       " 'mini_pickle': 'samples/all_trajs.mini.pkl',\n",
       " 'llm_json': 'samples/all_trajs.json',\n",
       " 'schema': 'samples/all_trajs.schema.md',\n",
       " 'keys': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trajs = \"all_trajs.pkl\"\n",
    "\n",
    "all_trajs_results = make_pickle_sample(\n",
    "        all_trajs,\n",
    "        output_dir=\"samples\",\n",
    "        target_bytes=1_500_000,\n",
    "        df_max_rows=3000,\n",
    "        random_state=42\n",
    ")\n",
    "\n",
    "all_trajs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a10dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pip-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
